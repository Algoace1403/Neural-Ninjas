# üöÄ Quick Start Guide - AI-Powered Pipeline

## üì¶ Installation (5 minutes)

### Step 1: Install Dependencies

```bash
cd "/Users/aks/Downloads/pipeline (1)"
pip install -r requirements.txt
```

**Note:** This will download ~500MB of ML models. First time takes 5-10 minutes.

### Step 2: Start MongoDB

**Option A: Local MongoDB**
```bash
# If you have MongoDB installed locally
mongod
```

**Option B: Docker**
```bash
docker run -d -p 27017:27017 --name mongodb mongo:latest
```

**Option C: MongoDB Atlas (Cloud)**
- Update `config.py` with your Atlas connection string

---

## ‚úÖ Test the Installation

### Test 1: Check Dependencies

```bash
python -c "
import transformers
import sklearn
import pandas as pd
from sentence_transformers import SentenceTransformer
print('‚úÖ All AI dependencies installed!')
"
```

### Test 2: Test AI Schema Inference

```bash
python -c "
from ai_schema_inference import SemanticFieldClassifier

classifier = SemanticFieldClassifier()
result = classifier.classify_field_name('price')
print(f'Field: price -> Category: {result}')

result = classifier.classify_field_name('email')
print(f'Field: email -> Category: {result}')

print('‚úÖ AI Schema Inference working!')
"
```

---

## üéØ Run the Application

### Option 1: Flask Web App (Recommended for First Test)

```bash
python app.py
```

Then open: `http://localhost:5001`

**What to test:**
1. Upload `sample.json` or `sample.csv`
2. See AI-detected schema with semantic categories
3. Check statistics (duplicates, anomalies, quality score)
4. View AI recommendations

### Option 2: Analytics Dashboard

```bash
python dashboard.py
```

Then open: `http://localhost:8050`

**What to explore:**
1. Overview tab - Data statistics
2. Trends tab - Field trends (needs historical data)
3. Schema Analysis - Schema evolution
4. Recommendations - AI suggestions

### Option 3: REST API

```bash
# Start the API
python app.py

# In another terminal, test the API
curl http://localhost:5001/api/v1/health
```

---

## üß™ Test with Sample Data

### Create Test Data

Create a file `test_ai_data.json`:

```json
[
  {
    "name": "John Doe",
    "email": "john@example.com",
    "age": 30,
    "salary": 75000,
    "hire_date": "2020-01-15",
    "department": "Engineering"
  },
  {
    "name": "Jane Smith",
    "email": "jane@example.com",
    "age": 28,
    "salary": 82000,
    "hire_date": "2019-05-20",
    "department": "Engineering"
  },
  {
    "name": "Bob Johnson",
    "email": "bob@example.com",
    "age": 35,
    "salary": 95000,
    "hire_date": "2018-03-10",
    "department": "Management"
  }
]
```

### Upload via Web Interface

1. Go to `http://localhost:5001`
2. Upload `test_ai_data.json`
3. Observe:
   - ‚úÖ `name` detected as `personal_name`
   - ‚úÖ `email` detected as `contact`
   - ‚úÖ `age` detected as `integer`
   - ‚úÖ `salary` detected as `monetary`
   - ‚úÖ `hire_date` detected as `temporal`
   - ‚úÖ `department` detected as `category`

### Upload via API

```bash
curl -X POST http://localhost:5001/api/v1/ingest \
  -H "Content-Type: application/json" \
  -d @test_ai_data.json
```

---

## üîç Test AI Features

### Test 1: Missing Value Prediction

Create `test_missing.json`:

```json
[
  {"name": "Alice", "age": 25, "salary": 60000},
  {"name": "Bob", "age": null, "salary": 70000},
  {"name": "Charlie", "age": 35, "salary": null}
]
```

Upload and see:
- ‚úÖ Missing age predicted based on other values
- ‚úÖ Missing salary predicted intelligently

### Test 2: Anomaly Detection

Create `test_anomaly.json`:

```json
[
  {"product": "Laptop", "price": 1200},
  {"product": "Mouse", "price": 25},
  {"product": "Keyboard", "price": 80},
  {"product": "Cable", "price": 99999}
]
```

Upload and see:
- ‚úÖ Cable with price 99999 flagged as anomaly
- ‚úÖ `_is_anomaly: true` field added

### Test 3: Change Detection

1. Upload same data twice with different values
2. Check `/api/v1/analytics/changes` to see detected changes

### Test 4: Semantic Understanding

Create `test_semantic.json` with various field types:

```json
[
  {
    "user_id": "12345",
    "full_name": "John Doe",
    "contact_email": "john@example.com",
    "phone_number": "+1234567890",
    "website_url": "https://example.com",
    "account_balance": 5000.50,
    "registration_date": "2023-01-15",
    "is_active": true,
    "user_category": "premium"
  }
]
```

Upload and verify semantic categories detected correctly.

---

## üìä Test Analytics

### Get Trends (requires historical data)

```bash
curl http://localhost:5001/api/v1/analytics/trends
```

### Get Recommendations

```bash
curl http://localhost:5001/api/v1/analytics/recommendations
```

### Get Schema Versions

```bash
curl http://localhost:5001/api/v1/schema/versions
```

### Get Data Summary

```bash
curl http://localhost:5001/api/v1/data/summary
```

---

## üåê Test Web Scraping (Optional)

### Scrape JSON API

```bash
curl -X POST http://localhost:5001/api/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "source_type": "url_json",
    "config": {
      "url": "https://jsonplaceholder.typicode.com/users",
      "json_path": ""
    }
  }'
```

### Scrape HTML Table

```bash
curl -X POST http://localhost:5001/api/v1/ingest \
  -H "Content-Type: application/json" \
  -d '{
    "source_type": "url_table",
    "config": {
      "url": "https://www.w3schools.com/html/html_tables.asp"
    }
  }'
```

---

## üéØ Expected Results

### After Uploading Data You Should See:

1. **Schema Detection**
   ```
   ü§ñ AI-Processed 3 records, inserted 3 with 6 fields!
   Schema v1 saved.
   ```

2. **Semantic Categories**
   - Each field classified into 1 of 12 semantic categories
   - Confidence scores shown

3. **Data Quality Score**
   - Overall quality score (0-100)
   - Based on completeness and validity

4. **Recommendations**
   - AI-generated suggestions for improvements

5. **Statistics**
   - Total records processed
   - Duplicates removed
   - Anomalies detected
   - Changes detected

---

## üêõ Troubleshooting

### Issue: "No module named 'transformers'"

```bash
pip install transformers sentence-transformers torch
```

### Issue: "MongoDB connection failed"

- Check MongoDB is running: `mongod --version`
- Verify connection string in `config.py`

### Issue: "Model download takes too long"

- Normal on first run (downloads ~500MB)
- Use faster internet or download models separately

### Issue: "Dashboard not loading"

```bash
pip install dash plotly dash-bootstrap-components
```

### Issue: "Import errors"

```bash
# Reinstall all dependencies
pip install -r requirements.txt --force-reinstall
```

---

## üìà Performance Tips

1. **Batch Size**: Adjust `BATCH_SIZE` in `config.py` (default: 100)
2. **Model Selection**: Use smaller models if RAM limited
3. **Disable Features**: Comment out ML features in `app.py` if not needed

---

## ‚ú® What Makes This Special?

1. **100% Automatic** - No manual schema definition needed
2. **Intelligent** - Understands field meanings, not just types
3. **Self-improving** - Learns from historical data
4. **Production Ready** - REST API, batch processing, error handling
5. **Extensible** - Easy to add new data sources and models

---

## üéì Demo Tips

1. **Start with simple data** - Show basic schema detection
2. **Add complexity gradually** - Missing values, anomalies
3. **Show real-time dashboard** - Live analytics
4. **Highlight AI features** - Semantic understanding, recommendations
5. **Demo API** - Show programmatic access

---

## üìö Next Steps

1. ‚úÖ Upload various data types
2. ‚úÖ Explore the dashboard
3. ‚úÖ Test API endpoints
4. ‚úÖ Try web scraping
5. ‚úÖ Review AI recommendations
6. ‚úÖ Check MongoDB collections

---

**You're all set! üöÄ**

The pipeline is now 95% complete with full AI capabilities.
Upload any data and watch the AI work its magic! ‚ú®
