# ğŸš€ Dynamic ETL Pipeline - AI Powered

Upload any JSON/CSV file. The pipeline automatically infers schema with **AI type detection**, cleans every record, **detects changes**, removes duplicates, and loads everything into MongoDB with full versioning!

## âœ¨ Key Features

### ğŸ¤– AI-Powered Intelligence
- **Smart Type Detection**: Automatically detects integers, floats, emails, dates, URLs, booleans
- **Schema Evolution**: Schema adapts as new fields appear
- **Data Normalization**: Standardizes dates, emails, numbers automatically

### ğŸ” Advanced Tracking
- **Schema Versioning**: Every unique schema saved with timestamps
- **Change Detection**: Tracks changes in key fields (price, discount, score, etc.)
- **Deduplication**: Automatically prevents duplicate records
- **Full Audit Trail**: Complete history in MongoDB

### ğŸ’ Production Ready
- **Batch Processing**: Handles large datasets efficiently
- **Error Handling**: Robust error management
- **Comprehensive Logging**: All operations logged with timestamps
- **Beautiful UI**: Modern dashboard with real-time statistics

## ğŸ¯ How To Run

### Prerequisites
- Python 3.7+
- MongoDB running locally or connection URI

### Installation
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Start MongoDB (if not running)
mongod
# OR use MongoDB Compass

# 3. Run the application
python app.py

# 4. Open browser
http://127.0.0.1:5000
```

## ğŸ“Š Testing the Features

### Test 1: Type Detection
1. Upload `test_data_complete.json`
2. See AI-detected types: integer, float, email, date, url, boolean
3. Check MongoDB Compass â†’ `schema_versions` collection

### Test 2: Deduplication
1. Upload `test_data_complete.json`
2. Upload the same file again
3. Notice "X duplicates skipped" message

### Test 3: Change Detection
1. Upload `test_data_complete.json`
2. Upload `test_data_modified.json` (has changed prices/scores)
3. See changes table showing old vs new values
4. Check MongoDB â†’ `data_changes` collection

### Test 4: Schema Evolution
1. Upload `sample.json` (different structure)
2. Upload `test_data_complete.json` (different fields)
3. See schema version increment (v1 â†’ v2)

## ğŸ“ MongoDB Collections

The system creates 3 collections:

1. **`entries`** - Main data storage with metadata
2. **`schema_versions`** - Schema history and versions
3. **`data_changes`** - Field change tracking

## ğŸ¨ What Makes This Special?

âœ… **Zero Configuration** - No manual schema definition needed
âœ… **Intelligent** - AI-powered type inference
âœ… **Adaptive** - Handles evolving data structures
âœ… **Smart** - Automatic deduplication
âœ… **Tracking** - Full change detection and history
âœ… **Production-Grade** - Error handling, logging, batch processing
âœ… **Beautiful UI** - Modern dashboard with statistics

## ğŸ”§ Configuration

Edit `config.py` to customize:
- MongoDB connection URI
- Database and collection names
- Batch processing size

## ğŸ“ˆ Use Cases

Perfect for:
- ğŸ›’ E-commerce price/product tracking
- ğŸ“Š Multi-source data aggregation
- ğŸŒ Web scraping pipelines
- ğŸ“¡ API data collection
- ğŸ”„ Real-time data synchronization

## ğŸš€ Future Enhancements

See `FEATURES.md` for:
- Detailed feature documentation
- What's implemented vs vision
- Hackathon demo script
- Enhancement roadmap

## ğŸ“ Tech Stack

- **Backend**: Flask, Python 3
- **Database**: MongoDB
- **Processing**: Batch processing with configurable size
- **Type Detection**: Regex + intelligent inference
- **UI**: HTML5, CSS3, Jinja2 templates

---

**Built for Hackathon** | Scalable | Production-Ready | AI-Powered ğŸ¤–
